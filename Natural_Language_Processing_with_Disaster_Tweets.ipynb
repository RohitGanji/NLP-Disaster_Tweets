{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing with Disaster Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohitGanji/NLP-Disaster_Tweets/blob/main/Natural_Language_Processing_with_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2B8gApdMC8"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18GCVXEceFK9",
        "outputId": "2ec07f1d-dc86-47ec-e3c3-99b2dc4cdcb5"
      },
      "source": [
        "# Import csv data from drive\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "# Helper functions\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def clean_data(text):\n",
        "  # Removing urls\n",
        "  text = re.sub(r\"http\\S+\", \"\", text).strip()\n",
        "  # Removing @username\n",
        "  text = re.sub(r\"@\\S+\", \"\", text).strip()\n",
        "  # Removing non-ASCII\n",
        "  text = text.encode(\"ascii\", \"ignore\").decode().strip()\n",
        "  # Removing punctuation marks\n",
        "  no_punct = [word for word in text if word not in string.punctuation]\n",
        "  text = \"\".join(no_punct).strip()\n",
        "  # Lower the text\n",
        "  text = text.lower()\n",
        "  # Lemmatization\n",
        "  lemm_text = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "  text = \" \".join(lemm_text).strip()\n",
        "  # Removing stopwords\n",
        "  stopwords = [word for word in text.split() if word not in stopword]\n",
        "  text = \" \".join(stopwords)\n",
        "  # remove additional space from string \n",
        "  text = re.sub(' +', ' ', text)\n",
        "  return text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8_qJxUkC6Sx"
      },
      "source": [
        "# Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMEb-vH4gmcM"
      },
      "source": [
        "# Clean the data\n",
        "train[\"text\"] = train.apply(lambda x: clean_data(x[\"text\"]), axis=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "X4mqATws_UEH",
        "outputId": "70b4b963-d278-4028-8f5c-812b6edccb02"
      },
      "source": [
        "# Shuffle the data\n",
        "train_shuffled = train.sample(frac=1)\n",
        "train_shuffled.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6839</th>\n",
              "      <td>9796</td>\n",
              "      <td>trapped</td>\n",
              "      <td>Greensburg, PA</td>\n",
              "      <td>know new release week blood call ancient evil ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2536</th>\n",
              "      <td>3640</td>\n",
              "      <td>desolation</td>\n",
              "      <td>Quilmes , Arg</td>\n",
              "      <td>desperation dislocation separation condemnatio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4261</th>\n",
              "      <td>6055</td>\n",
              "      <td>heat%20wave</td>\n",
              "      <td>Somewhere in Spain</td>\n",
              "      <td>well seen thats bummer weve heat wave tho 43c ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4190</th>\n",
              "      <td>5953</td>\n",
              "      <td>hazard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>davis drug guide nurse judith hopfer deglin ap...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7168</th>\n",
              "      <td>10272</td>\n",
              "      <td>war%20zone</td>\n",
              "      <td>We're All Mad Here</td>\n",
              "      <td>packing ct aka room look like war zone</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... target\n",
              "6839   9796  ...      0\n",
              "2536   3640  ...      1\n",
              "4261   6055  ...      1\n",
              "4190   5953  ...      0\n",
              "7168  10272  ...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2N6wjcHCK8Z",
        "outputId": "a5927dbd-b726-4dea-ced1-73f8fd5c0e7f"
      },
      "source": [
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1)\n",
        "\n",
        "print(train_sentences.shape, val_sentences.shape, train_labels.shape, val_labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6851,) (762,) (6851,) (762,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdYWjcmeCmbo",
        "outputId": "d322b7a5-6265-4055-a8ba-592b601fb70f"
      },
      "source": [
        "train_sentences[:10], train_labels"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['add familia arson squad', 'found sunflower explosion walk',\n",
              "        'hope get batista bombed lauren',\n",
              "        'first night retainer quite weird better get used wear every single night next year least',\n",
              "        'date release ep03 desolation set stay tuned info finalise schedule alt electro rock comingsoon',\n",
              "        'lie though pay oldest sometimes like first get car curfew freedom donthate',\n",
              "        'twitter update pretty much wrecked app',\n",
              "        'photo weallheartonedirection wouldnt let david electrocute im asshole',\n",
              "        'something kissing mass murderer doesnt sit right feel ok',\n",
              "        'diretube information egypt cyprus greece agreed fightterrorism'],\n",
              "       dtype=object), array([0, 1, 0, ..., 0, 0, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHBLvrdUCs14"
      },
      "source": [
        "# Model Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAZVtTVqEuBZ"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def f1_metric(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er11rOWsC-dI"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create USE layer\n",
        "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shape=[],\n",
        "                                        dtype=tf.string,\n",
        "                                        trainable=False,\n",
        "                                        name=\"USE\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abhaQ_R3F9Lp"
      },
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_1_USE\")\n",
        "\n",
        "# Comile the model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\", f1_metric])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBQk2R_8Ex9x",
        "outputId": "1710bf6d-6cf3-43dd-c084-d617740402ab"
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit(train_sentences,\n",
        "                        train_labels,\n",
        "                        epochs=10,\n",
        "                        validation_data=(val_sentences, val_labels))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "215/215 [==============================] - 8s 12ms/step - loss: 0.4995 - accuracy: 0.7749 - f1_metric: 0.7162 - val_loss: 0.4643 - val_accuracy: 0.7743 - val_f1_metric: 0.7124\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4256 - accuracy: 0.8069 - f1_metric: 0.7619 - val_loss: 0.4613 - val_accuracy: 0.7769 - val_f1_metric: 0.7130\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4100 - accuracy: 0.8173 - f1_metric: 0.7707 - val_loss: 0.4588 - val_accuracy: 0.7769 - val_f1_metric: 0.7194\n",
            "Epoch 4/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4006 - accuracy: 0.8228 - f1_metric: 0.7741 - val_loss: 0.4609 - val_accuracy: 0.7769 - val_f1_metric: 0.7271\n",
            "Epoch 5/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3905 - accuracy: 0.8251 - f1_metric: 0.7800 - val_loss: 0.4695 - val_accuracy: 0.7795 - val_f1_metric: 0.7349\n",
            "Epoch 6/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3806 - accuracy: 0.8323 - f1_metric: 0.7892 - val_loss: 0.4632 - val_accuracy: 0.7848 - val_f1_metric: 0.7357\n",
            "Epoch 7/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3692 - accuracy: 0.8377 - f1_metric: 0.7920 - val_loss: 0.4627 - val_accuracy: 0.7835 - val_f1_metric: 0.7227\n",
            "Epoch 8/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3583 - accuracy: 0.8422 - f1_metric: 0.7958 - val_loss: 0.4611 - val_accuracy: 0.7848 - val_f1_metric: 0.7343\n",
            "Epoch 9/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3460 - accuracy: 0.8483 - f1_metric: 0.8058 - val_loss: 0.4624 - val_accuracy: 0.7913 - val_f1_metric: 0.7394\n",
            "Epoch 10/10\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3332 - accuracy: 0.8551 - f1_metric: 0.8160 - val_loss: 0.4701 - val_accuracy: 0.7822 - val_f1_metric: 0.7288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHV8QFBYJc4j",
        "outputId": "2123a049-2985-4d73-ebf7-1352a1d463c2"
      },
      "source": [
        "model.evaluate(val_sentences, val_labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 8ms/step - loss: 0.4701 - accuracy: 0.7822 - f1_metric: 0.7288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4701420068740845, 0.7821522355079651, 0.7288091778755188]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mx8zV3WFX6o"
      },
      "source": [
        "## Generating submit csv\n",
        "test_sentences = test.apply(lambda x: clean_data(x[\"text\"]), axis=1).to_numpy()\n",
        "preds = tf.squeeze(tf.round(model.predict(test_sentences)))\n",
        "preds = tf.cast(preds, dtype=tf.int32)\n",
        "sample_submission[\"target\"] = preds\n",
        "sample_submission.to_csv(\"submit.csv\", index=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DczVD_6Gi05",
        "outputId": "d347c20a-00d8-45fa-b5cd-80e38ac29029"
      },
      "source": [
        "sample_submission[\"target\"].value_counts()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2005\n",
              "1    1258\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}